1、kubernetes 安转

1.1 环境安装前配置
# hostname 设置
cat << EOF >> /etc/hosts 
# 2019-10-22 add for kubernetes
192.168.100.99  master.scnebula.com
192.168.100.81  app081.scnebula.com
192.168.100.82  app082.scnebula.com
192.168.100.83  app083.scnebula.com
192.168.100.84  app084.scnebula.com
EOF

# 关闭防火墙
systemctl stop firewalld
systemctl disable firewalld
setenforce 0
sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config

# 关闭内存swap
swapoff -a
sed -i 's/.*swap.*/#&/' /etc/fstab

# 配置k8s 内核参数
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
或
sysctl --system

# 配置 kubernetes、docker 的yum源
yum install -y wget
mkdir /etc/yum.repos.d/bak && mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repo
wget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo
yum clean all && yum makecache

# 时间同步
yum install -y chrony
systemctl enable chronyd.service && systemctl start chronyd.service && systemctl status chronyd.service
chronyc sources

1.2 dockers安装

#!/bin/bash

# 在 master 节点和 worker 节点都要执行

# 安装 docker
# 参考文档如下
# https://docs.docker.com/install/linux/docker-ce/centos/ 
# https://docs.docker.com/install/linux/linux-postinstall/

# 卸载旧版本
yum remove -y docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-selinux \
docker-engine-selinux \
docker-engine

# 设置 yum repository
yum install -y yum-utils \
device-mapper-persistent-data \
lvm2
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum clean all && yum makecache

# 安装并启动 docker
yum list docker-ce --showduplicates | sort
yum install -y docker-ce-18.09.7 docker-ce-cli-18.09.7 containerd.io

# docker 目录重置
cat << EOF >>/etc/docker/daemon.json
{
"registry-mirrors": ["https://q2hy3fzi.mirror.aliyuncs.com"],
"data-root": "/data/db/docker",
"insecure-registries":["harbor.scnebula.com"]
}
EOF

systemctl enable docker && systemctl start docker

# 修改docker Cgroup Driver为systemd
# # 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
# # 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd
# 如果不修改，在添加 worker 节点时可能会碰到如下错误
# [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". 
# Please follow the guide at https://kubernetes.io/docs/setup/cri/
sed -i "s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g" /usr/lib/systemd/system/docker.service


systemctl daemon-reload
systemctl restart docker

docker version


服务器之间 无密码访问
ssh-keygen

work节点执行：
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

 yum install -y ipvsadm

1.3 kubernetes 镜像装备

# 配置K8S的yum源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 卸载旧版本
yum remove -y kubelet kubeadm kubectl




# 设置 docker 镜像，提高 docker 镜像下载速度和稳定性
# 如果您访问 https://hub.docker.io 速度非常稳定，亦可以跳过这个步骤
curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io

# 重启 docker，并启动 kubelet
systemctl enable kubelet && systemctl start kubelet

1.4 安装kubeadm kubectl kubelet
# 安装kubelet、kubeadm、kubectl
yum install -y kubelet-1.14.8 kubeadm-1.14.8 kubectl-1.14.8 --disableexcludes=kubernetes
systemctl enable kubelet && systemctl start kubelet

#############################################################################################
#############################################################################################
                        以上命令是需要在所有的节点执行
#############################################################################################
#############################################################################################

#############################################################################################
#############################################################################################
                        master节点执行
#############################################################################################
#############################################################################################

# 命令查看依赖需要安装的镜像列表
kubeadm config images list
# 生成默认kubeadm.conf文件
kubeadm config print init-defaults > kubeadm.conf

sed -i "s/imageRepository: .*/imageRepository: registry.aliyuncs.com/google_containers/g" kubeadm.conf

# 下载所需镜像
docker tag registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.8 k8s.gcr.io/kube-apiserver:v1.14.8
docker tag registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.8 k8s.gcr.io/kube-controller-manager:v1.14.8
docker tag registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.8 k8s.gcr.io/kube-scheduler:v1.14.8
docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.14.8 k8s.gcr.io/kube-proxy:v1.14.8
docker tag registry.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1
docker tag registry.aliyuncs.com/google_containers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
docker tag registry.aliyuncs.com/google_containers/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1

docker rmi registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.8
docker rmi registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.8
docker rmi registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.8
docker rmi registry.aliyuncs.com/google_containers/kube-proxy:v1.14.8
docker rmi registry.aliyuncs.com/google_containers/pause:3.1
docker rmi registry.aliyuncs.com/google_containers/etcd:3.3.10
docker rmi registry.aliyuncs.com/google_containers/coredns:1.3.1

kubeadm init --kubernetes-version=v1.14.8 --pod-network-cidr=172.22.0.0/16 --apiserver-advertise-address=192.168.100.99
或者(建议使用如下：)
kubeadm init --config kubeadm.conf

kubeadm join 192.168.100.99:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:379602092d48a0910122c33a3d83086f9cd57c0fb44c7af87d855e6d1393e75e
# 检查master状态
kubectl get cs

kubectl get pods --all-namespaces

1.5 caclio网络安装
systemctl disable NetworkManager
systemctl stop NetworkManager

curl https://docs.projectcalico.org/v3.10/manifests/calico.yaml -O
# 编辑 修改ip网段


# qidong
kubectl apply -f calico.yaml

# 修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”
kubectl edit cm kube-proxy -n kube-system
# 重启各个节点上的kube-proxypod
kubectl get pod -n kube-system | grep kube-proxy | awk '{system("kubectl delete pod "$1" -n kube-system")}'


1.5 安装kubernetes-dashboard

mkdir -p /etc/kubernetes/certs $ cd /etc/kubernetes/certs
openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048
openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.key
rm -rf dashboard.pass.key$ openssl req -new -key dashboard.key -out dashboard.csr
openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt

kubectl create secret generic kubernetes-dashboard-certs --from-file=/etc/kubernetes/certs -n kube-system

docker pull registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0
docker tag registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard:v1.10.0
docker rmi registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0

# 安装完成以后 master节点无法访问，执行下：
# 参考地址： https://blog.csdn.net/fei79534672/article/details/78710858
iptables -P FORWARD ACCEPT

# kubectl describe node master.scnebula.com |grep Taints
Taints:             node-role.kubernetes.io/master:NoSchedule

# 去掉
kubectl taint node master.scnebula.com node-role.kubernetes.io/master:NoSchedule-
# 加上
kubectl taint node master.scnebula.com node-role.kubernetes.io/master=:NoSchedule
kubectl taint node master.scnebula.com node-role.kubernetes.io/master=:PreferNoSchedule



###############################
# k8s新增节点步骤
1、环境配置
2、docker install
3、install kubeadm kubectl kubelet
4、pull images (kubernetes dashboard)
5、join master node

主要命令如下：
yum install bash-completion
systemctl stop firewalld
systemctl disable firewalld
setenforce 0
sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
swapoff -a
sed -i 's/.*swap.*/#&/' /etc/fstab
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
yum install -y wget
mkdir /etc/yum.repos.d/bak && mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repo
wget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo
yum clean all && yum makecache
yum -y install chrony
systemctl enable chronyd.service && systemctl start chronyd.service && systemctl status chronyd.service
chronyc sources
yum remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine
yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum list docker-ce --showduplicates | sort
yum install -y docker-ce-18.09.7 docker-ce-cli-18.09.7 containerd.io
mkdir -p /data/db/docker /etc/docker

cat << EOF >>/etc/docker/daemon.json
{
"registry-mirrors": ["https://q2hy3fzi.mirror.aliyuncs.com"],
"data-root": "/data/db/docker",
"insecure-registries":["harbor.scnebula.com"]
}
EOF

systemctl enable docker && systemctl start docker
sed -i "s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g" /usr/lib/systemd/system/docker.service
systemctl daemon-reload
systemctl restart docker
docker info |grep Cgroup
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
yum install -y ipvsadm
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum remove -y kubelet kubeadm kubectl
yum install -y kubelet-1.14.8 kubeadm-1.14.8 kubectl-1.14.8 --disableexcludes=kubernetes
systemctl enable kubelet && systemctl start kubelet
systemctl disable NetworkManager && systemctl stop NetworkManager
docker pull registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0
docker tag registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard:v1.10.0
docker rmi registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0
kubeadm join 192.168.100.99:6443 --token k0cwzn.0gpslbj740kgbaj4     --discovery-token-ca-cert-hash sha256:49286815b1456a43093cc18a03ce3ab8b7cfa67713e1cdd22f0c622e54e3bed6




admin-secret:
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXJwajQyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMzQ3NzA4Ni03MTg5LTRkZTEtODkxNS1jYTg4ZDgwMjVjZDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.GNOtJJk4eJKbbv6AWIhUYmPTQ-3g-cHTztAMn_YfE71HRlHS6IgqwGoMvpfZL_XgBp8DG1helvyzUOqyGBCFdQrTFvMZdmRPjMrpf8Uzo69q8j_s3Xut6QF81bO3AIx6Dj23wqnvsurAWEKw3s4T5I1Q_2jLTFsz9uM3N9QkqCRz6VtJ6FlscYFGRRuAVsMbXX4c5Epd8BwuYdoOaPcW5D4JMeWm6xiu5mNkV0C4AhiOYvQO-1RugkHjSOaUxJ_BAmhjlacRPvhqxss427V247wpDWBEX2WD25cBxPRKO_hOsZIJd1pqL5bTp91-KudavZhTOa0Fm-7ctEeb8wFChw

pangu-user:
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJucy1wYW5ndSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwYW5ndS11c2VyLXRva2VuLW45OHNqIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InBhbmd1LXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2MjFkNTIxNi0xNDVhLTRhN2UtYWM1ZC04MWFhOTg5NjY4YjEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6bnMtcGFuZ3U6cGFuZ3UtdXNlciJ9.Fu0Z1pPZjz30AbCrPw7s6XxO_h6jzeDFuFPD4x0MPNT6yb-sX4UaGKqZJmh48xpgpc0LkaFxySmZuOxCiBx6Ebmwrp945m_atfDgw3XnnojlD1ty7Kb-5oB_w9vQQy06rkfLi0Km8vC0iLjgngBDi6GlPrFDdxPVq46FmTvGwQLn32vYgDQGwiXW6tsq1Cr-tSZ0ZeE9OK_JmjlV9_K7cApiPt9OuLj6E07VETBXkhByhJLPT7VU7bi3KlYM-VcJF_pidR1Baiz-NE245MnPJKwCrJ5toP75rcvZKXQ8wwtO8b948pY9J9LNu1vDFZPO-QbNaCFBEe3G2bAHdFTO-w



kubectl create clusterrolebinding system-node-role-bound --clusterrole=system:node --group=system:nodes

kubectl create clusterrolebinding gitlab-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts --namespace=dev
kubectl create clusterrolebinding pangu-admin --clusterrole=system:node --group=system:serviceaccounts --namespace=ns-pangu


kubectl delete serviceaccount pangu -n ns-pangu


kubectl create rolebinding pangu-admin --clusterrole=admin --serviceaccount=pangu:pangu-admin -n ns-pangu

kubectl delete rolebinding pangu-admin -n ns-pangu


# dockers https harbor配置

复制certs的crt证书到目标机器
目录 /etc/dockers/certs.d/harbor.scnebula.com/ca.crt

同时把内容复制到bunle下：
 cat harbor.scnebula.com/ca.cr

 vi /etc/pki/tls/certs/ca-bundle.crt


 ##############################################################

# 版本升级  1.14 → 1.15
yum list --showduplicates kubeadm --disableexcludes=kubernetes
# 首先更新kubeadm 
yum install kubeadm-1.15.5-0 --disableexcludes=kubernetes
# 修改配置文件
kubeadm.conf 修改对应版本号
# 查看升级计划
kubeadm upgrade plan
# download images from aliyun
kubeadm config images pull --config=kubeadm.conf

# 升级

# 安装对应版本
yum install -y kubelet-1.15.5-0 --disableexcludes=kubernetes
yum install -y kubectl-1.15.5-0 --disableexcludes=kubernetes
systemctl daemon-reload

systemctl restart kubelet

# 逐一drain works node
kubectl drain app084.scnebula.com --ignore-daemonsets
kubectl get nodes
# 查看pods
kubectl get pods --all-namespaces -o wide
  ---升级次节点
    yum install -y kubeadm-1.15.5-0 --disableexcludes=kubernetes
    yum install -y kubelet-1.15.5-0 --disableexcludes=kubernetes
    yum install -y kubectl-1.15.5-0 --disableexcludes=kubernetes
    
  # upgrade 
   kubeadm upgrade node 
systemctl daemon-reload && systemctl restart kubelet

# 取消状态标记(Master EX)
kubectl uncordon app084.scnebula.com



MountVolume.SetUp failed for volume "default-token-nhbjc" : secret "default-token-nhbjc" not found
kubectl -n kube-system create secret generic etcd-certs --from-file=/etc/kubernetes/pki/etcd/server.crt --from-file=/etc/kubernetes/pki/etcd/server.key

